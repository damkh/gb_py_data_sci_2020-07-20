1) Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?
   Macro-усреднение оказывается НЕинформативным для несбалансированных выборок, так как метрики (Prec, Rec, Fb и т.д.) вычисляются для каждого класса и только потом усредняются. При micro-усреднении характеристики (TP, FP, FN) вычисляются для каждого класса и усредняются, после чего вычисляются метрики. Таким образом micro учитывает величины характеристика каждого класса. Взвешенное усреденение - это усреднение с весами (TP и TN), чтобы учесть разбалансированность. При взвешенном методе метрики вычилсяются для каждого класса (как и для macro). Но все же я не совсем понял разность между micro и weighted, оба метода учитывают разбалансированность
2) В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
    Сама библиотека XGBoost основана на фреймворке GBM, но имеет ряд улучшений: возможность распараллеливания на CPU и GPU, оптимизирована работа с памятью, реализована в переносимой архитектуре для возможности применения в разных средах (https://xgboost.readthedocs.io/en/latest/tutorials/model.html).
    При построении модели xgboost (в отличие от классического GBM) используется регуляризованное формирование модели для предотвращения переобучения. xgboost может работать только с числовыми данными.
    LigthGBM и CatBoost еще более продвинутые библиотке по произоводительности и лучше работают на больших датасетах. При построении модели lightgbm для выбора критерия разбиения используется GOSS - техника выделения объектов с наибольшим значением градиента. Также реализована возможность работы с sparse-признаками (много неошибочных пропусков) - позволяет агрегировать подобные признаки в один (для ускорения обучения). Модели catboost хорошо заточены на работу с категориальными признаками. 

   
 
